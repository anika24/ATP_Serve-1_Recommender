{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_players_many_points(df):\n",
    "    \"\"\"\n",
    "    Find players with at least x points in train and y points in test.\n",
    "    \"\"\"\n",
    "    df_res = df.groupby('server').filter(lambda x: (x['data_split_2024'] == 'train').sum() > 3650 and (x['data_split_2024'] == 'test').sum() > 100)\n",
    "    return df_res['server'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541611\n",
      "358650\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/anika/Desktop/BEM:EC 120/generated_datasets/atp_features_0518.csv')\n",
    "df = df.drop(['server_bp_saved_recent_matches', 'returner_depth_recent_matches_1st/2nd', 'returner_depth_recent_matches_wide', 'returner_depth_recent_matches_T', 'returner_depth_recent_matches_body'], axis=1)\n",
    "df = df.dropna()\n",
    "df = df[df['first_shot_loc'] != 'unknown']\n",
    "df = df[df['first_serve_attempt'] != 'unknown']\n",
    "df = df[df['serve_loc'] != 'unknown']\n",
    "print(len(df))\n",
    "\n",
    "# If any values have a space in them, make it a _\n",
    "df = df.replace(' ', '_', regex=True)\n",
    "\n",
    "# make everything lowercase\n",
    "df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "df = df[df['first_shot_loc'] != 'no_first_shot']\n",
    "print(len(df))\n",
    "\n",
    "# check if first/second serve by seeing if 'first_serve_attempt' is 'made_first_serve'\n",
    "df['serve_type'] = df.apply(lambda row: 1 if row['first_serve_attempt'] == 'made_first_serve' else 2, axis=1)\n",
    "\n",
    "df['s+1'] = df['serve_loc'] + '_' + df['first_shot_loc']\n",
    "df['side_s+1'] = df.apply(lambda row: f\"{row['serve_loc']}_{row['first_shot_loc']}_{row['court_side']}\", axis=1)\n",
    "df['1st/2nd_s+1'] = df['serve_loc'] + '_' + df['first_shot_loc'] + '_' + df['serve_type'].astype(str) + '_' + df['court_side']\n",
    "df['side_serve_loc'] = df['serve_loc'] + '_on_' + df['court_side']\n",
    "df['side_first_shot_loc'] = df['first_shot_loc'] + '_on_' + df['court_side']\n",
    "df['1st/2nd_serve_loc'] = df['serve_loc'] + '_on_' + df['court_side'] + '_' + df['serve_type'].astype(str)\n",
    "df['1st/2nd_first_shot_loc'] = df['first_shot_loc'] + '_on_' + df['court_side'] + '_' + df['serve_type'].astype(str)\n",
    "df['date'] = pd.to_datetime(df['match_id'].str.split('-').str[0], format='%Y%m%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_win_rates(df, n=5):\n",
    "    \"\"\"\n",
    "    Add rolling win rates based on s+1, serve_loc, and first_shot_loc.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by=['server', 'date']).copy()\n",
    "\n",
    "    def compute_rolling_win_pct(df, group_col, prefix):\n",
    "        summary = (\n",
    "            df.groupby(['server', group_col, 'match_id'])\n",
    "              .agg(total_pts=('won_pt', 'count'), won_pts=('won_pt', 'sum'))\n",
    "              .reset_index()\n",
    "        )\n",
    "        summary['rolling_win_pct'] = (\n",
    "            summary\n",
    "            .groupby(['server', group_col])\n",
    "            .apply(lambda g: g[['won_pts', 'total_pts']]\n",
    "                   .shift()\n",
    "                   .rolling(n, min_periods=1)\n",
    "                   .sum()\n",
    "                   .eval('won_pts / total_pts'))\n",
    "            .reset_index(level=[0,1], drop=True)\n",
    "        )\n",
    "        pivoted = (\n",
    "            summary.pivot(index=['server', 'match_id'], \n",
    "                          columns=group_col, \n",
    "                          values='rolling_win_pct')\n",
    "                   .reset_index()\n",
    "        )\n",
    "        pivoted.columns = ['server', 'match_id'] + [f'{prefix}_{c}' for c in pivoted.columns[2:]]\n",
    "        return pivoted\n",
    "\n",
    "    # Compute for each feature\n",
    "    s1_pivot = compute_rolling_win_pct(df, '1st/2nd_s+1', 'win_pct_s1')\n",
    "    print('done1')\n",
    "    serve_loc_pivot = compute_rolling_win_pct(df, '1st/2nd_serve_loc', 'win_pct_serve')\n",
    "    print('done2')\n",
    "    shot_loc_pivot = compute_rolling_win_pct(df, '1st/2nd_first_shot_loc', 'win_pct_first_shot')\n",
    "    print('done3')\n",
    "\n",
    "    # Drop old win_pct_* columns if any\n",
    "    df = df.drop(columns=[col for col in df.columns if col.startswith('win_pct_')], errors='ignore')\n",
    "\n",
    "    # Merge all\n",
    "    df = df.merge(s1_pivot, on=['server', 'match_id'], how='left')\n",
    "    df = df.merge(serve_loc_pivot, on=['server', 'match_id'], how='left')\n",
    "    df = df.merge(shot_loc_pivot, on=['server', 'match_id'], how='left')\n",
    "\n",
    "    # Fill NaNs\n",
    "    win_pct_cols = [col for col in df.columns if col.startswith('win_pct_')]\n",
    "    df[win_pct_cols] = df[win_pct_cols].fillna(0.5)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done1\n",
      "done2\n",
      "done3\n"
     ]
    }
   ],
   "source": [
    "df = add_rolling_win_rates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Add features based on interaction between categorical variables for locations\n",
    "    for the serve and first shot.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Columns to one-hot encode\n",
    "\n",
    "    one_hot_cols = [\n",
    "        'surface', 'hand_combo', 'court_side', 'first_serve_attempt',\n",
    "        'serve_loc', 'first_shot_loc', 'is_break_point', 'is_game_point',\n",
    "    ]\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=one_hot_cols)\n",
    "\n",
    "    # Scale numerical variables\n",
    "    scaler = StandardScaler()\n",
    "    numerical_to_scale = ['game_advantage', 'set_advantage', 'point_advantage', 'pts_in_game', 'pts_in_match', \n",
    "                          'height', 'height_difference', 'rally_lengths_won_pts',\n",
    "                          'rally_lengths_lost_pts', 'win_streak_server', 'win_streak_opponent']\n",
    "    \n",
    "    df[numerical_to_scale] = scaler.fit_transform(df[numerical_to_scale])\n",
    "\n",
    "    mean_return = df[['wide_returner_win_recent_matches', \n",
    "                    'body_returner_win_recent_matches',\n",
    "                    'T_returner_win_recent_matches']].mean(axis=1)\n",
    "        \n",
    "    df['wide_return_advantage'] = df['serve_loc_wide'] * (df['wide_returner_win_recent_matches'] - mean_return)\n",
    "    df['body_return_advantage'] = df['serve_loc_body'] * (df['body_returner_win_recent_matches'] - mean_return)\n",
    "    df['T_return_advantage'] = df['serve_loc_t'] * (df['T_returner_win_recent_matches'] - mean_return)\n",
    "    \n",
    "    df['returner_success_on_this_serve_recent_matches'] = (\n",
    "        df['serve_loc_wide'] * df['wide_return_advantage'] + \n",
    "        df['serve_loc_body'] * df['body_return_advantage'] +\n",
    "        df['serve_loc_t'] * df['body_return_advantage']\n",
    "    )\n",
    "\n",
    "    df['interaction_returns_wide'] = df['serve_loc_wide'] * df['wide_returner_win_recent_matches']\n",
    "    df['interaction_returns_body'] = df['serve_loc_body'] * df['body_returner_win_recent_matches']\n",
    "    df['interaction_returns_T'] = df['serve_loc_t'] * df['T_returner_win_recent_matches']\n",
    "\n",
    "    df['opponent_error_rate_from_+1_loc'] = (\n",
    "        df['first_shot_loc_deuce_court'] * df['op_pct_errors_deuce_side'] + \n",
    "        df['first_shot_loc_ad_court'] * df['op_pct_errors_ad_side'] +\n",
    "        df['first_shot_loc_middle'] * df['op_pct_errors_middle']\n",
    "    )\n",
    "\n",
    "    df['interaction_+1_deuce'] = df['first_shot_loc_deuce_court'] * df['op_pct_errors_deuce_side']\n",
    "    df['interaction_+1_ad'] = df['first_shot_loc_ad_court'] * df['op_pct_errors_ad_side']\n",
    "    df['interaction_+1_middle'] = df['first_shot_loc_middle'] * df['op_pct_errors_middle']\n",
    "\n",
    "    df['opponent_pct_shots_are_errors_from_+1_loc'] = (\n",
    "        df['first_shot_loc_deuce_court'] * df['op_pct_shots_errors_deuce_side'] + \n",
    "        df['first_shot_loc_ad_court'] * df['op_pct_shots_errors_ad_side'] +\n",
    "        df['first_shot_loc_middle'] * df['op_pct_shots_errors_middle']\n",
    "    )\n",
    "\n",
    "    df['interaction_+1_deuce_shots'] = df['first_shot_loc_deuce_court'] * df['op_pct_shots_errors_deuce_side']\n",
    "    df['interaction_+1_ad_shots'] = df['first_shot_loc_ad_court'] * df['op_pct_shots_errors_ad_side']\n",
    "    df['interaction_+1_middle_shots'] = df['first_shot_loc_middle'] * df['op_pct_shots_errors_middle']\n",
    "    \n",
    "    df['rolling_server_success_on_this_serve'] = (\n",
    "        df['serve_loc_wide'] * df['wide_server_win_rate'] +\n",
    "        df['serve_loc_body'] * df['body_server_win_rate'] +\n",
    "        df['serve_loc_t'] * df['T_server_win_rate']\n",
    "    )\n",
    "\n",
    "    mean_serve = df[['wide_server_win_rate', 'body_server_win_rate', 'T_server_win_rate']].mean(axis=1)\n",
    "\n",
    "    df['interaction_serve_wide'] = df['serve_loc_wide'] * df['wide_server_win_rate']\n",
    "    df['interaction_serve_body'] = df['serve_loc_body'] * df['body_server_win_rate']\n",
    "    df['interaction_serve_T'] = df['serve_loc_t'] * df['T_server_win_rate']\n",
    "\n",
    "    df['wide_server_advantage'] = df['serve_loc_wide'] * (df['wide_server_win_rate'] - mean_serve)\n",
    "    df['body_server_advantage'] = df['serve_loc_body'] * (df['body_server_win_rate'] - mean_serve)\n",
    "    df['T_server_advantage'] = df['serve_loc_t'] * (df['T_server_win_rate'] - mean_serve)\n",
    "\n",
    "    df['gp_won_recent_matches'] = df['is_game_point_True'] * df['server_gp_won_recent_matches']\n",
    "    df['s+1_success_rate_recent_matches'] = df.apply(lambda row: row[f'win_pct_s1_{row[\"1st/2nd_s+1\"]}'], axis=1)\n",
    "    df['serve_success_rate_recent_matches'] = df.apply(lambda row: row[f'win_pct_serve_{row[\"1st/2nd_serve_loc\"]}'], axis=1)\n",
    "    df['first_shot_success_rate_recent_matches'] = df.apply(lambda row: row[f'win_pct_first_shot_{row[\"1st/2nd_first_shot_loc\"]}'], axis=1)\n",
    "\n",
    "    # Label encode server and opponent (for embeddings)\n",
    "    label_encoders = {}\n",
    "    for col in ['server', 'opponent', 'side_serve_loc', 'side_first_shot_loc', 'side_s+1', '1st/2nd_first_shot_loc',\n",
    "                '1st/2nd_s+1', '1st/2nd_serve_loc']:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    return df, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "players_lst = find_players_many_points(df)\n",
    "print(len(players_lst))\n",
    "df = df[df['server'].isin(players_lst)]\n",
    "df_preprocessed, label_encoders = add_interaction_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "categorical_cols = ['side_serve_loc', 'side_first_shot_loc']\n",
    "\n",
    "# After one-hot encoding, collect all new columns\n",
    "one_hot_prefixes = ['surface_']\n",
    "\n",
    "numerical_cols = [col for col in df_preprocessed.columns if any(col.startswith(prefix) for prefix in one_hot_prefixes)] + [\n",
    "    'is_break_point_True',\n",
    "    'is_game_point_True',\n",
    "    'first_serve_attempt_made_first_serve',\n",
    "    'court_side_deuce',\n",
    "    'point_advantage',\n",
    "    'set_advantage',\n",
    "    'game_advantage',\n",
    "    'pts_in_match',\n",
    "    'win_streak_server',\n",
    "    'height',\n",
    "    'height_difference',\n",
    "    '1st/2nd_server_win_rate',\n",
    "    'returner_win_1st/2nd_recent_matches',\n",
    "    'same_serve_last',\n",
    "    'same_shot_last',\n",
    "    's1_success_rate_whole_match',\n",
    "    'good_pts_server_minus_opp',\n",
    "    'wide_return_advantage', 'body_return_advantage', 'T_return_advantage',\n",
    "    'interaction_+1_deuce', 'interaction_+1_ad', 'opponent_error_rate_from_middle',\n",
    "    'interaction_serve_wide', 'interaction_serve_body', 'interaction_serve_T',\n",
    "    'gp_won_recent_matches',\n",
    "    's+1_success_rate_recent_matches',\n",
    "    'serve_success_rate_recent_matches',\n",
    "]\n",
    "\n",
    "target_cols = ['won_pt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TennisDataset(Dataset):\n",
    "    def __init__(self, df, categorical_cols, numerical_cols, target_cols):\n",
    "        self.categorical_data = df[categorical_cols].values.astype(np.int64)\n",
    "        self.numerical_data = df[numerical_cols].values.astype(np.float32)\n",
    "        self.targets = df[target_cols].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_cat = torch.tensor(self.categorical_data[idx], dtype=torch.long)\n",
    "        x_num = torch.tensor(self.numerical_data[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)  # shape: [num_targets]\n",
    "        return x_cat, x_num, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskServePlusOneModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, num_numerical, hidden_units):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Embedding(categories, size),\n",
    "            ) for categories, size in embedding_sizes\n",
    "        ])\n",
    "        self.emb_dim = sum(e[0].embedding_dim for e in self.embeddings)\n",
    "\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim + num_numerical, hidden_units[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # Add dropout\n",
    "            nn.Linear(hidden_units[0], hidden_units[1]),\n",
    "            nn.ReLU(),\n",
    "        )      \n",
    "\n",
    "        # One output head per task\n",
    "        self.output_won = nn.Linear(hidden_units[1], 1)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        if x_cat is not None:\n",
    "            emb_outs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "            emb_cat = torch.cat(emb_outs, dim=1)\n",
    "        else:\n",
    "            emb_cat = torch.zeros(x_num.size(0), self.emb_dim, device=x_num.device)\n",
    "\n",
    "        x = torch.cat([emb_cat, x_num], dim=1)\n",
    "        x = self.shared_fc(x)\n",
    "        return self.output_won(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion_won, device='cpu'):\n",
    "    model.train().to(device)  # Ensure model is on correct device\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for x_cat, x_num, y in dataloader:\n",
    "        x_cat, x_num, y = x_cat.to(device), x_num.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        if x_cat.size(1) == 0: # check is x_cat tensor is empty\n",
    "            out_won = model(None, x_num)\n",
    "        else:\n",
    "            out_won = model(x_cat, x_num)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion_won(out_won.squeeze(), y[:, 0])\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item() * x_cat.size(0)  # Weight by batch size\n",
    "        total_samples += x_cat.size(0)\n",
    "\n",
    "    return total_loss / total_samples  # Return average loss per sample\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion_won, device='cpu'):\n",
    "    model.eval().to(device)\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    won_preds = []\n",
    "    won_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_num, y in dataloader:\n",
    "            x_cat, x_num, y = x_cat.to(device), x_num.to(device), y.to(device)\n",
    "            if x_cat.size(1) == 0: # check is x_cat tensor is empty\n",
    "                out_won = model(None, x_num)\n",
    "            else:\n",
    "                out_won = model(x_cat, x_num)\n",
    "\n",
    "            # Ensure shapes are compatible\n",
    "            if out_won.dim() == 0:  # If out_won is a scalar\n",
    "                out_won = out_won.unsqueeze(0)  # Add batch dimension\n",
    "            else:\n",
    "                out_won = out_won.squeeze()  # Remove extra dimensions if necessary\n",
    "\n",
    "            y_target = y[:, 0].squeeze()  # Ensure target has the correct shape\n",
    "            \n",
    "            loss = criterion_won(out_won, y_target)\n",
    "            total_loss += loss.item() * x_cat.size(0)\n",
    "            total_samples += x_cat.size(0)\n",
    "            \n",
    "            won_preds.extend(torch.sigmoid(out_won).view(-1).cpu().tolist())\n",
    "            won_targets.extend(y_target.view(-1).cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    \n",
    "    won_metrics = calculate_binary_metrics(\n",
    "        np.array(won_preds),\n",
    "        np.array(won_targets),\n",
    "        threshold=0.5\n",
    "    )\n",
    "        \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'won_metrics': won_metrics,\n",
    "    }\n",
    "\n",
    "def calculate_binary_metrics(preds, targets, threshold=0.5):\n",
    "    # Apply threshold to get binary predictions\n",
    "    binary_preds = (preds >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(targets, binary_preds) # Accuracy is the ratio of correct predictions to total predictions\n",
    "    precision = precision_score(targets, binary_preds, zero_division=0) # Precision (combos that were predicted to win actually won) \n",
    "    recall = recall_score(targets, binary_preds, zero_division=0) # Recall (combos that won were actually predicted to win)\n",
    "    f1 = f1_score(targets, binary_preds, zero_division=0) # F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "    roc_auc = roc_auc_score(targets, preds) # ability to distinguish between classes\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pred_probs': preds,   # Raw probabilities for further analysis\n",
    "        'targets': targets,    # Ground truth labels\n",
    "        'ratio': np.mean(binary_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_full(categorical_cols, numerical_cols, target_cols, embedding_size, lr, batch_size, epochs, hidden_units = [64, 32], pos_weight = True, notes = ''):\n",
    "    embedding_sizes = [(len(label_encoders[col].classes_), min(embedding_size, (len(label_encoders[col].classes_) + 1) // 2))\n",
    "                    for col in categorical_cols]\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    df_train = df_preprocessed[df_preprocessed['data_split_2024'] == 'train'].drop('data_split_2024', axis=1)\n",
    "    df_fine_tune = df_preprocessed[df_preprocessed['data_split_2024'] == 'test'].drop('data_split_2024', axis=1)\n",
    "\n",
    "    dataset_train = TennisDataset(df_train, categorical_cols, numerical_cols, target_cols)\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dataset_fine_tune = TennisDataset(df_fine_tune, categorical_cols, numerical_cols, target_cols)\n",
    "    dataloader_fine_tune = DataLoader(dataset_fine_tune, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = MultiTaskServePlusOneModel(embedding_sizes, num_numerical=len(numerical_cols), hidden_units=hidden_units)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # L2 regularization\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    criterion_won = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Because we have a sligtly unbalanced dataset\n",
    "    if pos_weight:\n",
    "        pos_ratio = df_train[target_cols[0]].mean()\n",
    "        neg_ratio = 1 - pos_ratio\n",
    "        pos_weight = torch.tensor(neg_ratio / pos_ratio)\n",
    "        criterion_won = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, dataloader_train, optimizer, criterion_won)\n",
    "        val_results = evaluate_model(model, dataloader_fine_tune, criterion_won)\n",
    "        val_loss = val_results['loss']\n",
    "        val_metrics = val_results['won_metrics']\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_metrics['accuracy']:.4f}, Val Precision = {val_metrics['precision']:.4f}, Val Recall = {val_metrics['recall']:.4f}, Val F1 = {val_metrics['f1']:.4f}, Val Ratio = {val_metrics['ratio']:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(player, categorical_cols, numerical_cols, target_cols, embedding_size, lr, batch_size, epochs, hidden_units = [64, 32], pos_weight = True, notes = '', results_dict = {}, model = None):\n",
    "    df_player = df_preprocessed[df_preprocessed['server'] == player]\n",
    "\n",
    "    embedding_sizes = [(len(label_encoders[col].classes_), min(embedding_size, (len(label_encoders[col].classes_) + 1) // 2))\n",
    "                    for col in categorical_cols]\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    print(len(df_player))\n",
    "    df_train = df_player[df_player['data_split_2024'] == 'train'].drop('data_split_2024', axis=1)\n",
    "    df_test = df_player[df_player['data_split_2024'] == 'test'].drop('data_split_2024', axis=1)\n",
    "    df_test_fs = df_test[df_test['first_serve_attempt_made_first_serve'] == 1]\n",
    "    df_test_ss = df_test[df_test['first_serve_attempt_made_first_serve'] == 0]\n",
    "    df_fine_tune = df_player[df_player['data_split_2024'] == 'test'].drop('data_split_2024', axis=1)\n",
    "\n",
    "    dataset_train = TennisDataset(df_train, categorical_cols, numerical_cols, target_cols)\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dataset_test = TennisDataset(df_test, categorical_cols, numerical_cols, target_cols)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    dataset_test_fs = TennisDataset(df_test_fs, categorical_cols, numerical_cols, target_cols)\n",
    "    dataloader_test_fs = DataLoader(dataset_test_fs, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    dataset_test_ss = TennisDataset(df_test_ss, categorical_cols, numerical_cols, target_cols)\n",
    "    dataloader_test_ss = DataLoader(dataset_test_ss, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    dataset_fine_tune = TennisDataset(df_fine_tune, categorical_cols, numerical_cols, target_cols)\n",
    "    dataloader_fine_tune = DataLoader(dataset_fine_tune, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion_won = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if model is None:\n",
    "        model = MultiTaskServePlusOneModel(embedding_sizes, num_numerical=len(numerical_cols), hidden_units=hidden_units)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Because we have a sligtly unbalanced dataset\n",
    "        if pos_weight:\n",
    "            pos_ratio = df_train[target_cols[0]].mean()\n",
    "            neg_ratio = 1 - pos_ratio\n",
    "            pos_weight = torch.tensor(neg_ratio / pos_ratio)\n",
    "            criterion_won = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = train_model(model, dataloader_train, optimizer, criterion_won)\n",
    "            val_results = evaluate_model(model, dataloader_fine_tune, criterion_won)\n",
    "            val_loss = val_results['loss']\n",
    "            val_metrics = val_results['won_metrics']\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        \n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_metrics['accuracy']:.4f}, Val Precision = {val_metrics['precision']:.4f}, Val Recall = {val_metrics['recall']:.4f}, Val F1 = {val_metrics['f1']:.4f}, Val Ratio = {val_metrics['ratio']:.4f}\")\n",
    "\n",
    "    test_results_all = evaluate_model(model, dataloader_test, criterion_won)\n",
    "    test_results_fs = evaluate_model(model, dataloader_test_fs, criterion_won)\n",
    "    test_results_ss = evaluate_model(model, dataloader_test_ss, criterion_won)\n",
    "    test_loss = test_results_all['loss']\n",
    "    test_metrics = test_results_all['won_metrics']\n",
    "\n",
    "    print(f\"Test Loss = {test_loss:.4f}, Test Accuracy = {test_metrics['accuracy']:.4f}, Test Precision = {test_metrics['precision']:.4f}, Test Recall = {test_metrics['recall']:.4f}, Test F1 = {test_metrics['f1']:.4f}, Test Ratio = {test_metrics['ratio']:.4f}\")\n",
    "\n",
    "    results_dict[player]['test_metrics']['all'] = test_results_all['won_metrics']\n",
    "    results_dict[player]['test_metrics']['fs'] = test_results_fs['won_metrics']\n",
    "    results_dict[player]['test_metrics']['ss'] = test_results_ss['won_metrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6933, Val Loss = 0.6777, Val Accuracy = 0.5763, Val Precision = 0.5811, Val Recall = 0.9432, Val F1 = 0.7191, Val Ratio = 0.9335\n",
      "Epoch 2: Train Loss = 0.6833, Val Loss = 0.6753, Val Accuracy = 0.5791, Val Precision = 0.5833, Val Recall = 0.9387, Val F1 = 0.7195, Val Ratio = 0.9255\n",
      "Epoch 3: Train Loss = 0.6817, Val Loss = 0.6740, Val Accuracy = 0.5838, Val Precision = 0.5878, Val Recall = 0.9254, Val F1 = 0.7189, Val Ratio = 0.9055\n",
      "Epoch 4: Train Loss = 0.6810, Val Loss = 0.6734, Val Accuracy = 0.5836, Val Precision = 0.5912, Val Recall = 0.8950, Val F1 = 0.7120, Val Ratio = 0.8707\n",
      "Epoch 5: Train Loss = 0.6802, Val Loss = 0.6734, Val Accuracy = 0.5841, Val Precision = 0.5933, Val Recall = 0.8801, Val F1 = 0.7088, Val Ratio = 0.8532\n",
      "Epoch 6: Train Loss = 0.6796, Val Loss = 0.6722, Val Accuracy = 0.5854, Val Precision = 0.5931, Val Recall = 0.8893, Val F1 = 0.7116, Val Ratio = 0.8624\n",
      "Epoch 7: Train Loss = 0.6796, Val Loss = 0.6725, Val Accuracy = 0.5855, Val Precision = 0.5962, Val Recall = 0.8652, Val F1 = 0.7060, Val Ratio = 0.8346\n",
      "Epoch 8: Train Loss = 0.6789, Val Loss = 0.6720, Val Accuracy = 0.5857, Val Precision = 0.5952, Val Recall = 0.8743, Val F1 = 0.7082, Val Ratio = 0.8449\n",
      "Epoch 9: Train Loss = 0.6785, Val Loss = 0.6718, Val Accuracy = 0.5850, Val Precision = 0.5947, Val Recall = 0.8745, Val F1 = 0.7079, Val Ratio = 0.8458\n",
      "Epoch 10: Train Loss = 0.6783, Val Loss = 0.6717, Val Accuracy = 0.5862, Val Precision = 0.5967, Val Recall = 0.8656, Val F1 = 0.7064, Val Ratio = 0.8343\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "results_dict = defaultdict(lambda : defaultdict(lambda: defaultdict(list))) # defaultdict\n",
    "players_lst = find_players_many_points(df_preprocessed)\n",
    "\n",
    "model = train_model_full(categorical_cols, numerical_cols, target_cols, embedding_size=32, lr=1e-4,\n",
    "                         batch_size=32, epochs=10, hidden_units=[128, 16], pos_weight=False, notes = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6320\n",
      "Test Loss = 0.6678, Test Accuracy = 0.6001, Test Precision = 0.6098, Test Recall = 0.8941, Test F1 = 0.7251, Test Ratio = 0.8647\n",
      "5987\n",
      "Test Loss = 0.6609, Test Accuracy = 0.6084, Test Precision = 0.6180, Test Recall = 0.8800, Test F1 = 0.7261, Test Ratio = 0.8399\n",
      "6064\n",
      "Test Loss = 0.6839, Test Accuracy = 0.5673, Test Precision = 0.5682, Test Recall = 0.7576, Test F1 = 0.6494, Test Ratio = 0.7051\n",
      "4534\n",
      "Test Loss = 0.6822, Test Accuracy = 0.5390, Test Precision = 0.5400, Test Recall = 0.7687, Test F1 = 0.6344, Test Ratio = 0.7407\n",
      "7374\n",
      "Test Loss = 0.6696, Test Accuracy = 0.5881, Test Precision = 0.6067, Test Recall = 0.8708, Test F1 = 0.7151, Test Ratio = 0.8522\n",
      "5524\n",
      "Test Loss = 0.6697, Test Accuracy = 0.5817, Test Precision = 0.5947, Test Recall = 0.8666, Test F1 = 0.7054, Test Ratio = 0.8420\n",
      "10227\n",
      "Test Loss = 0.6769, Test Accuracy = 0.5754, Test Precision = 0.5856, Test Recall = 0.8030, Test F1 = 0.6773, Test Ratio = 0.7610\n",
      "8297\n",
      "Test Loss = 0.6909, Test Accuracy = 0.5640, Test Precision = 0.5535, Test Recall = 0.8756, Test F1 = 0.6782, Test Ratio = 0.8303\n",
      "5858\n",
      "Test Loss = 0.6743, Test Accuracy = 0.5812, Test Precision = 0.5917, Test Recall = 0.8447, Test F1 = 0.6959, Test Ratio = 0.8099\n",
      "11987\n",
      "Test Loss = 0.6816, Test Accuracy = 0.5628, Test Precision = 0.5626, Test Recall = 0.8982, Test F1 = 0.6919, Test Ratio = 0.8724\n",
      "8872\n",
      "Test Loss = 0.6627, Test Accuracy = 0.6069, Test Precision = 0.6325, Test Recall = 0.8511, Test F1 = 0.7257, Test Ratio = 0.8222\n",
      "13581\n",
      "Test Loss = 0.6725, Test Accuracy = 0.5888, Test Precision = 0.6013, Test Recall = 0.8974, Test F1 = 0.7201, Test Ratio = 0.8797\n",
      "8349\n",
      "Test Loss = 0.6628, Test Accuracy = 0.6184, Test Precision = 0.6197, Test Recall = 0.9135, Test F1 = 0.7385, Test Ratio = 0.8694\n",
      "4884\n",
      "Test Loss = 0.6914, Test Accuracy = 0.5188, Test Precision = 0.5132, Test Recall = 0.7750, Test F1 = 0.6175, Test Ratio = 0.7569\n",
      "7668\n",
      "Test Loss = 0.6659, Test Accuracy = 0.6106, Test Precision = 0.6078, Test Recall = 0.9073, Test F1 = 0.7279, Test Ratio = 0.8570\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model for each player\n",
    "for player in players_lst:\n",
    "    train_and_evaluate(player, categorical_cols, numerical_cols, target_cols, embedding_size=32, lr=1e-4, batch_size=32, epochs=10, hidden_units=[128, 16],\n",
    "                       pos_weight=False, results_dict = results_dict, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score (all): 0.6952, Std F1 score (all): 0.0359\n",
      "Mean Accuracy score (all): 0.5808, Std Accuracy score (all): 0.0266\n",
      "Mean F1 score (fs): 0.7423, Std F1 score (fs): 0.0282\n",
      "Mean Accuracy score (fs): 0.6068, Std Accuracy score (fs): 0.0330\n",
      "Mean F1 score (ss): 0.6039, Std F1 score (ss): 0.0719\n",
      "Mean Accuracy score (ss): 0.5427, Std Accuracy score (ss): 0.0327\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Results\n",
    "f1_scores_all = [results_dict[player]['test_metrics']['all']['f1'] for player in players_lst]\n",
    "f1_scores_fs = [results_dict[player]['test_metrics']['fs']['f1'] for player in players_lst]\n",
    "f1_scores_ss = [results_dict[player]['test_metrics']['ss']['f1'] for player in players_lst]\n",
    "\n",
    "acc_scores_all = [results_dict[player]['test_metrics']['all']['accuracy'] for player in players_lst]\n",
    "acc_scores_fs = [results_dict[player]['test_metrics']['fs']['accuracy'] for player in players_lst]\n",
    "acc_scores_ss = [results_dict[player]['test_metrics']['ss']['accuracy'] for player in players_lst]\n",
    "\n",
    "print(f\"Mean F1 score (all): {np.mean(f1_scores_all):.4f}, Std F1 score (all): {np.std(f1_scores_all):.4f}\")\n",
    "print(f\"Mean Accuracy score (all): {np.mean(acc_scores_all):.4f}, Std Accuracy score (all): {np.std(acc_scores_all):.4f}\")\n",
    "\n",
    "print(f\"Mean F1 score (fs): {np.mean(f1_scores_fs):.4f}, Std F1 score (fs): {np.std(f1_scores_fs):.4f}\")\n",
    "print(f\"Mean Accuracy score (fs): {np.mean(acc_scores_fs):.4f}, Std Accuracy score (fs): {np.std(acc_scores_fs):.4f}\")\n",
    "\n",
    "print(f\"Mean F1 score (ss): {np.mean(f1_scores_ss):.4f}, Std F1 score (ss): {np.std(f1_scores_ss):.4f}\")\n",
    "print(f\"Mean Accuracy score (ss): {np.mean(acc_scores_ss):.4f}, Std Accuracy score (ss): {np.std(acc_scores_ss):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
